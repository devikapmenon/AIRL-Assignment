{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP/yHMzc9TjPFh6DUeIf5Ea"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"f1b553d8"},"source":["!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n","!pip install -q timm"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%bash\n","cat > train.py <<'PY'\n","# (SCRIPT STARTS HERE: see the code block later in this file)\n","PY"],"metadata":{"id":"yvikGOl3kmxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":387},"id":"702058eb","executionInfo":{"status":"error","timestamp":1759522692399,"user_tz":-330,"elapsed":114,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"9e3ef16f-0646-42fd-8cb9-566abcb45bf6"},"source":["# train.py — ViT for CIFAR-10 (single-file, Colab-ready)\n","# Usage example (in Colab):\n","# !python train.py --epochs 200 --batch-size 128 --lr 3e-3 --patch-size 4 --embed-dim 256 --depth 10 --num-heads 8\n","\n","import argparse\n","import math\n","import os\n","import random\n","from pathlib import Path\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import LambdaLR\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# -------------------------- Model pieces --------------------------\n","class PatchEmbed(nn.Module):\n","    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=256):\n","        super().__init__()\n","        assert img_size % patch_size == 0, 'Image dimensions must be divisible by patch size.'\n","        self.patch_size = patch_size\n","        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x):\n","        x = self.proj(x)                        # [B, embed_dim, H/ps, W/ps]\n","        x = x.flatten(2).transpose(1, 2)       # [B, num_patches, embed_dim]\n","        return x\n","\n","class MLP(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, dropout=0.):\n","        super().__init__()\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(hidden_features, in_features)\n","        self.drop = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_dropout=0., proj_dropout=0.):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_dropout)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]   # each: [B, nh, N, head_dim]\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","def drop_path(x, drop_prob: float = 0., training: bool = False):\n","    if drop_prob == 0. or not training:\n","        return x\n","    keep_prob = 1 - drop_prob\n","    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n","    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n","    random_tensor.floor_()\n","    output = x.div(keep_prob) * random_tensor\n","    return output\n","\n","class DropPath(nn.Module):\n","    def __init__(self, drop_prob=None):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        return drop_path(x, self.drop_prob, self.training)\n","\n","class Block(nn.Module):\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_dropout=attn_drop, proj_dropout=drop)\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = nn.LayerNorm(dim)\n","        mlp_hidden = int(dim * mlp_ratio)\n","        self.mlp = MLP(dim, hidden_features=mlp_hidden, dropout=drop)\n","\n","    def forward(self, x):\n","        x = x + self.drop_path(self.attn(self.norm1(x)))\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        return x\n","\n","class VisionTransformer(nn.Module):\n","    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10,\n","                 embed_dim=256, depth=10, num_heads=8, mlp_ratio=4., qkv_bias=True,\n","                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.):\n","        super().__init__()\n","        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n","        num_patches = self.patch_embed.num_patches\n","\n","        # class token + positional embeddings\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        # transformer blocks\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n","        self.blocks = nn.ModuleList([\n","            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n","                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i]) for i in range(depth)\n","        ])\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","        # classifier head\n","        self.head = nn.Linear(embed_dim, num_classes)\n","\n","        # init\n","        try:\n","            nn.init.trunc_normal_(self.pos_embed, std=0.02)\n","            nn.init.trunc_normal_(self.cls_token, std=0.02)\n","        except Exception:\n","            # older torch versions may not have trunc_normal_; fallback to normal_\n","            nn.init.normal_(self.pos_embed, std=0.02)\n","            nn.init.normal_(self.cls_token, std=0.02)\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            try:\n","                nn.init.trunc_normal_(m.weight, std=0.02)\n","            except Exception:\n","                nn.init.normal_(m.weight, std=0.02)\n","            if m.bias is not None:\n","                nn.init.zeros_(m.bias)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.zeros_(m.bias)\n","            nn.init.ones_(m.weight)\n","\n","    def forward(self, x):\n","        B = x.shape[0]\n","        x = self.patch_embed(x)                 # [B, N, C]\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)   # [B, 1+N, C]\n","        x = x + self.pos_embed\n","        x = self.pos_drop(x)\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","        cls = x[:, 0]\n","        x = self.head(cls)\n","        return x\n","\n","# -------------------------- Utilities --------------------------\n","def accuracy(output, target, topk=(1,)):\n","    maxk = max(topk)\n","    batch_size = target.size(0)\n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))\n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","        res.append(correct_k.mul_(100.0 / batch_size))\n","    return res\n","\n","class LabelSmoothingCrossEntropy(nn.Module):\n","    def __init__(self, smoothing=0.1):\n","        super().__init__()\n","        assert 0.0 <= smoothing < 1.0\n","        self.smoothing = smoothing\n","\n","    def forward(self, pred, target):\n","        logprobs = F.log_softmax(pred, dim=-1)\n","        nll = -logprobs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n","        smooth_loss = -logprobs.mean(dim=-1)\n","        loss = (1.0 - self.smoothing) * nll + self.smoothing * smooth_loss\n","        return loss.mean()\n","\n","class SimpleMixup:\n","    def __init__(self, alpha=0.8, device='cuda'):\n","        self.alpha = alpha\n","        self.device = device\n","\n","    def __call__(self, x, y):\n","        if self.alpha <= 0:\n","            return x, y\n","        lam = np.random.beta(self.alpha, self.alpha)\n","        batch_size = x.size(0)\n","        index = torch.randperm(batch_size).to(self.device)\n","        mixed_x = lam * x + (1 - lam) * x[index, :]\n","        y_a, y_b = y, y[index]\n","        return mixed_x, (y_a, y_b, lam)\n","\n","class MixupLoss:\n","    def __init__(self, criterion):\n","        self.criterion = criterion\n","\n","    def __call__(self, preds, targets):\n","        if isinstance(targets, tuple):\n","            y_a, y_b, lam = targets\n","            return lam * self.criterion(preds, y_a) + (1 - lam) * self.criterion(preds, y_b)\n","        else:\n","            return self.criterion(preds, targets)\n","\n","# cosine lr scheduler with warmup\n","def build_scheduler(optimizer, total_epochs, steps_per_epoch, lr, warmup_epochs=10):\n","    total_steps = total_epochs * steps_per_epoch\n","    warmup_steps = warmup_epochs * steps_per_epoch\n","\n","    def lr_lambda(current_step):\n","        if current_step < warmup_steps:\n","            return float(current_step) / float(max(1, warmup_steps))\n","        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n","        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n","\n","    return LambdaLR(optimizer, lr_lambda)\n","\n","# -------------------------- Training / Validation loops --------------------------\n","def train_one_epoch(model, loader, optimizer, device, epoch, loss_fn, scheduler=None, mixup_fn=None):\n","    model.train()\n","    running_loss = 0.0\n","    top1 = 0.0\n","    total = 0\n","    for i, (x, y) in enumerate(loader):\n","        x = x.to(device)\n","        y = y.to(device)\n","        if mixup_fn is not None:\n","            x, y = mixup_fn(x, y)\n","            # When Mixup is active, the target `y` is a tuple (y_a, y_b, lam).\n","            # We need to use the original labels y_a for calculating accuracy.\n","            y_for_accuracy = y[0]\n","        else:\n","            y_for_accuracy = y\n","\n","\n","        logits = model(x)\n","        loss = loss_fn(logits, y)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        # Use y_for_accuracy for accuracy calculation\n","        acc1 = accuracy(logits.detach().cpu(), y_for_accuracy.detach().cpu(), topk=(1,))[0]\n","        running_loss += loss.item() * x.size(0)\n","        top1 += acc1.item() * x.size(0)\n","        total += x.size(0)\n","\n","    return running_loss / total, top1 / total\n","\n","@torch.no_grad()\n","def validate(model, loader, device, loss_fn):\n","    model.eval()\n","    running_loss = 0.0\n","    top1 = 0.0\n","    total = 0\n","    for x, y in loader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = loss_fn(logits, y)\n","        acc1 = accuracy(logits.detach().cpu(), y.detach().cpu(), topk=(1,))[0]\n","        running_loss += loss.item() * x.size(0)\n","        top1 += acc1.item() * x.size(0)\n","        total += x.size(0)\n","    return running_loss / total, top1 / total\n","\n","# -------------------------- Main & argparsing --------------------------\n","def main(args):\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print('Device:', device)\n","\n","    # reproducibility (optional)\n","    if args.seed is not None:\n","        random.seed(args.seed)\n","        np.random.seed(args.seed)\n","        torch.manual_seed(args.seed)\n","        if device == 'cuda':\n","            torch.cuda.manual_seed_all(args.seed)\n","\n","    # data transforms\n","    normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n","    train_transforms = [\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","    ]\n","    if args.rand_augment:\n","        # torchvision's RandAugment requires reasonably recent torchvision\n","        try:\n","            train_transforms.append(transforms.RandAugment())\n","        except Exception:\n","            pass\n","    train_transforms += [transforms.ToTensor(), normalize]\n","    train_transform = transforms.Compose(train_transforms)\n","\n","    test_transform = transforms.Compose([transforms.ToTensor(), normalize])\n","\n","    # datasets\n","    train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","    test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n","\n","    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=min(8, os.cpu_count() or 1), pin_memory=True)\n","    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=min(8, os.cpu_count() or 1), pin_memory=True)\n","\n","    # model\n","    model = VisionTransformer(img_size=32, patch_size=args.patch_size, num_classes=10,\n","                              embed_dim=args.embed_dim, depth=args.depth, num_heads=args.num_heads,\n","                              mlp_ratio=args.mlp_ratio, drop_rate=args.dropout, attn_drop_rate=args.attn_dropout,\n","                              drop_path_rate=args.drop_path_rate)\n","    model.to(device)\n","\n","    # criterion\n","    if args.label_smoothing > 0:\n","        base_criterion = LabelSmoothingCrossEntropy(smoothing=args.label_smoothing)\n","    else:\n","        base_criterion = nn.CrossEntropyLoss()\n","\n","    if args.mixup_alpha > 0:\n","        mixup_fn = SimpleMixup(alpha=args.mixup_alpha, device=device)\n","        criterion = MixupLoss(base_criterion)\n","    else:\n","        mixup_fn = None\n","        criterion = base_criterion\n","\n","    # optimizer + scheduler\n","    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    steps_per_epoch = len(train_loader)\n","    scheduler = build_scheduler(optimizer, total_epochs=args.epochs, steps_per_epoch=steps_per_epoch, lr=args.lr, warmup_epochs=args.warmup_epochs)\n","\n","    # checkpoint directory\n","    os.makedirs(args.output_dir, exist_ok=True)\n","\n","    best_acc = 0.0\n","    global_step = 0\n","    for epoch in range(args.epochs):\n","        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device, epoch, criterion, scheduler=scheduler, mixup_fn=mixup_fn)\n","        val_loss, val_acc = validate(model, test_loader, device, base_criterion)\n","\n","        print(f\"Epoch {epoch+1}/{args.epochs} — train loss: {train_loss:.4f} — train acc: {train_acc:.2f}% — val loss: {val_loss:.4f} — val acc: {val_acc:.2f}%\")\n","\n","        # save best\n","        if val_acc > best_acc:\n","            best_acc = val_acc\n","            torch.save({'epoch': epoch+1, 'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'best_acc': best_acc}, os.path.join(args.output_dir, 'best.pth'))\n","\n","    print('Training finished. Best val acc: %.2f%%' % best_acc)\n","\n","    # Save the final model checkpoint\n","    torch.save({'epoch': args.epochs, 'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'best_acc': best_acc}, os.path.join(args.output_dir, 'final.pth'))\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='ViT CIFAR10 — single-file')\n","    parser.add_argument('--epochs', type=int, default=200)\n","    parser.add_argument('--batch-size', type=int, default=128)\n","    parser.add_argument('--lr', type=float, default=3e-3)\n","    parser.add_argument('--weight-decay', type=float, default=0.05)\n","    parser.add_argument('--warmup-epochs', type=int, default=10)\n","    parser.add_argument('--patch-size', type=int, default=4)\n","    parser.add_argument('--embed-dim', type=int, default=256)\n","    parser.add_argument('--depth', type=int, default=10)\n","    parser.add_argument('--num-heads', type=int, default=8)\n","    parser.add_argument('--mlp-ratio', type=float, default=4.0)\n","    parser.add_argument('--dropout', type=float, default=0.1)\n","    parser.add_argument('--attn-dropout', type=float, default=0.0)\n","    parser.add_argument('--drop-path-rate', type=float, default=0.1)\n","    parser.add_argument('--label-smoothing', type=float, default=0.1)\n","    parser.add_argument('--mixup-alpha', type=float, default=0.8)\n","    parser.add_argument('--rand-augment', action='store_true')\n","    parser.add_argument('--output-dir', type=str, default='./checkpoints')\n","    parser.add_argument('--seed', type=int, default=42)\n","    args = parser.parse_args()\n","\n","    main(args)# save final checkpoint\n","torch.save({'epoch': epoch+1, 'model_state': model.state_dict(), 'best_acc': best_acc},\n","           os.path.join(args.output_dir, 'final.pth'))\n","print(\"Final checkpoint saved:\", os.path.join(args.output_dir, 'final.pth'))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["usage: colab_kernel_launcher.py [-h] [--epochs EPOCHS]\n","                                [--batch-size BATCH_SIZE] [--lr LR]\n","                                [--weight-decay WEIGHT_DECAY]\n","                                [--warmup-epochs WARMUP_EPOCHS]\n","                                [--patch-size PATCH_SIZE]\n","                                [--embed-dim EMBED_DIM] [--depth DEPTH]\n","                                [--num-heads NUM_HEADS]\n","                                [--mlp-ratio MLP_RATIO] [--dropout DROPOUT]\n","                                [--attn-dropout ATTN_DROPOUT]\n","                                [--drop-path-rate DROP_PATH_RATE]\n","                                [--label-smoothing LABEL_SMOOTHING]\n","                                [--mixup-alpha MIXUP_ALPHA] [--rand-augment]\n","                                [--output-dir OUTPUT_DIR] [--seed SEED]\n","colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-792014bf-bc9e-44de-9666-cee6567772f8.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"2","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","source":["# Basic run (uses GPU). Tune args below as desired.\n","!python train.py --epochs 200 --batch-size 128 --lr 3e-3 --weight-decay 0.05 \\\n","--patch-size 4 --embed-dim 256 --depth 10 --num-heads 8 --mlp-ratio 4.0 --label-smoothing 0.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mHFwrzzimBKC","executionInfo":{"status":"ok","timestamp":1759436641469,"user_tz":-330,"elapsed":5227,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"789f0744-fced-4efa-b6e6-c9f698803a69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: train.py [-h] [--epochs EPOCHS] [--batch-size BATCH_SIZE] [--lr LR]\n","                [--weight-decay WEIGHT_DECAY] [--warmup-epochs WARMUP_EPOCHS]\n","                [--patch-size PATCH_SIZE] [--embed-dim EMBED_DIM]\n","                [--depth DEPTH] [--num-heads NUM_HEADS]\n","                [--mlp-ratio MLP_RATIO] [--dropout DROPOUT]\n","                [--attn-dropout ATTN_DROPOUT]\n","                [--drop-path-rate DROP_PATH_RATE]\n","                [--label-smoothing LABEL_SMOOTHING]\n","                [--mixup-alpha MIXUP_ALPHA] [--rand-augment]\n","                [--output-dir OUTPUT_DIR] [--seed SEED]\n","train.py: error: unrecognized arguments: --mlp-dim 512\n"]}]},{"cell_type":"code","source":["!pip install timm  # only if you use timm for schedulers, optional\n","!pip install torch torchvision  # usually preinstalled in Colab\n","\n","# 2. (Optional) Verify GPU\n","!nvidia-smi\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F_MnZEUynDHz","executionInfo":{"status":"ok","timestamp":1759436895628,"user_tz":-330,"elapsed":8213,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"5d6beab3-ae87-48d4-cdd4-dd2d766a586e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.19)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.19.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n","Thu Oct  2 20:28:14 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# =======================\n","# RUN TRAINING\n","# =======================\n","\n","!python train.py \\\n","  --epochs 100 \\\n","  --batch-size 128 \\\n","  --lr 3e-4 \\\n","  --weight-decay 0.05 \\\n","  --embed-dim 256 \\\n","  --mlp-ratio 4.0 \\\n","  --depth 8 \\\n","  --num-heads 8 \\\n","  --patch-size 4 \\\n","  --dropout 0.1 \\\n","  --attn-dropout 0.0 \\\n","  --label-smoothing 0.1 \\\n","  --rand-augment \\\n","  --output-dir ./vit_cifar10_out \\\n","  --seed 42"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zxLgbWb_nKCD","executionInfo":{"status":"ok","timestamp":1759444749087,"user_tz":-330,"elapsed":2854773,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"0d13a444-f1dc-4b03-f3a3-322b00b3561c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","Epoch 1/100 — train loss: 2.2272 — train acc: 14.57% — val loss: 2.0412 — val acc: 25.68%\n","Epoch 2/100 — train loss: 2.1510 — train acc: 18.04% — val loss: 1.9191 — val acc: 32.42%\n","Epoch 3/100 — train loss: 2.0968 — train acc: 20.71% — val loss: 1.8644 — val acc: 36.18%\n","Epoch 4/100 — train loss: 2.0437 — train acc: 23.68% — val loss: 1.7677 — val acc: 42.72%\n","Epoch 5/100 — train loss: 1.9889 — train acc: 24.64% — val loss: 1.6560 — val acc: 46.56%\n","Epoch 6/100 — train loss: 1.9413 — train acc: 27.40% — val loss: 1.5888 — val acc: 51.20%\n","Epoch 7/100 — train loss: 1.9015 — train acc: 28.32% — val loss: 1.5205 — val acc: 53.48%\n","Epoch 8/100 — train loss: 1.8864 — train acc: 29.37% — val loss: 1.4708 — val acc: 56.59%\n","Epoch 9/100 — train loss: 1.8646 — train acc: 29.53% — val loss: 1.4703 — val acc: 56.35%\n","Epoch 10/100 — train loss: 1.8577 — train acc: 30.38% — val loss: 1.4985 — val acc: 54.48%\n","Epoch 11/100 — train loss: 1.8352 — train acc: 31.61% — val loss: 1.3768 — val acc: 60.17%\n","Epoch 12/100 — train loss: 1.8160 — train acc: 30.61% — val loss: 1.3668 — val acc: 60.34%\n","Epoch 13/100 — train loss: 1.8038 — train acc: 31.96% — val loss: 1.3539 — val acc: 62.28%\n","Epoch 14/100 — train loss: 1.7948 — train acc: 32.38% — val loss: 1.3276 — val acc: 62.65%\n","Epoch 15/100 — train loss: 1.7707 — train acc: 31.13% — val loss: 1.3191 — val acc: 62.80%\n","Epoch 16/100 — train loss: 1.7799 — train acc: 34.63% — val loss: 1.2802 — val acc: 64.66%\n","Epoch 17/100 — train loss: 1.7394 — train acc: 35.72% — val loss: 1.2805 — val acc: 65.51%\n","Epoch 18/100 — train loss: 1.7534 — train acc: 34.89% — val loss: 1.2394 — val acc: 67.48%\n","Epoch 19/100 — train loss: 1.7370 — train acc: 34.06% — val loss: 1.2662 — val acc: 66.11%\n","Epoch 20/100 — train loss: 1.7352 — train acc: 34.72% — val loss: 1.2100 — val acc: 68.24%\n","Epoch 21/100 — train loss: 1.7086 — train acc: 35.78% — val loss: 1.1870 — val acc: 69.90%\n","Epoch 22/100 — train loss: 1.7027 — train acc: 35.05% — val loss: 1.2002 — val acc: 69.22%\n","Epoch 23/100 — train loss: 1.7137 — train acc: 35.19% — val loss: 1.1969 — val acc: 69.25%\n","Epoch 24/100 — train loss: 1.6868 — train acc: 38.03% — val loss: 1.1825 — val acc: 69.81%\n","Epoch 25/100 — train loss: 1.6690 — train acc: 36.44% — val loss: 1.1482 — val acc: 71.86%\n","Epoch 26/100 — train loss: 1.6605 — train acc: 35.95% — val loss: 1.1195 — val acc: 73.08%\n","Epoch 27/100 — train loss: 1.6664 — train acc: 34.91% — val loss: 1.1048 — val acc: 73.86%\n","Epoch 28/100 — train loss: 1.6641 — train acc: 37.43% — val loss: 1.0868 — val acc: 74.75%\n","Epoch 29/100 — train loss: 1.6627 — train acc: 38.16% — val loss: 1.0931 — val acc: 74.50%\n","Epoch 30/100 — train loss: 1.6358 — train acc: 37.11% — val loss: 1.0901 — val acc: 74.66%\n","Epoch 31/100 — train loss: 1.6345 — train acc: 38.92% — val loss: 1.0948 — val acc: 73.65%\n","Epoch 32/100 — train loss: 1.6255 — train acc: 39.58% — val loss: 1.0798 — val acc: 74.43%\n","Epoch 33/100 — train loss: 1.6172 — train acc: 36.71% — val loss: 1.0575 — val acc: 75.86%\n","Epoch 34/100 — train loss: 1.6299 — train acc: 38.95% — val loss: 1.0767 — val acc: 75.02%\n","Epoch 35/100 — train loss: 1.6204 — train acc: 38.94% — val loss: 1.0399 — val acc: 76.82%\n","Epoch 36/100 — train loss: 1.6018 — train acc: 37.85% — val loss: 1.0332 — val acc: 77.08%\n","Epoch 37/100 — train loss: 1.5891 — train acc: 38.02% — val loss: 1.0294 — val acc: 76.75%\n","Epoch 38/100 — train loss: 1.5849 — train acc: 41.16% — val loss: 1.0259 — val acc: 77.36%\n","Epoch 39/100 — train loss: 1.5820 — train acc: 38.74% — val loss: 1.0150 — val acc: 77.79%\n","Epoch 40/100 — train loss: 1.5811 — train acc: 38.59% — val loss: 1.0239 — val acc: 77.44%\n","Epoch 41/100 — train loss: 1.5613 — train acc: 37.85% — val loss: 0.9870 — val acc: 78.98%\n","Epoch 42/100 — train loss: 1.5835 — train acc: 37.96% — val loss: 1.0166 — val acc: 77.91%\n","Epoch 43/100 — train loss: 1.5787 — train acc: 38.88% — val loss: 0.9929 — val acc: 79.02%\n","Epoch 44/100 — train loss: 1.5491 — train acc: 39.37% — val loss: 0.9721 — val acc: 79.68%\n","Epoch 45/100 — train loss: 1.5423 — train acc: 39.17% — val loss: 0.9803 — val acc: 79.22%\n","Epoch 46/100 — train loss: 1.5323 — train acc: 41.67% — val loss: 0.9994 — val acc: 78.02%\n","Epoch 47/100 — train loss: 1.5529 — train acc: 40.91% — val loss: 0.9690 — val acc: 79.54%\n","Epoch 48/100 — train loss: 1.5737 — train acc: 39.44% — val loss: 0.9676 — val acc: 80.16%\n","Epoch 49/100 — train loss: 1.5177 — train acc: 41.06% — val loss: 0.9666 — val acc: 79.76%\n","Epoch 50/100 — train loss: 1.5160 — train acc: 42.43% — val loss: 0.9289 — val acc: 81.85%\n","Epoch 51/100 — train loss: 1.5244 — train acc: 42.67% — val loss: 0.9543 — val acc: 80.63%\n","Epoch 52/100 — train loss: 1.5019 — train acc: 42.92% — val loss: 0.9255 — val acc: 82.04%\n","Epoch 53/100 — train loss: 1.5222 — train acc: 41.78% — val loss: 0.9131 — val acc: 81.83%\n","Epoch 54/100 — train loss: 1.5081 — train acc: 42.32% — val loss: 0.9253 — val acc: 81.73%\n","Epoch 55/100 — train loss: 1.5234 — train acc: 40.93% — val loss: 0.9114 — val acc: 82.54%\n","Epoch 56/100 — train loss: 1.5419 — train acc: 39.51% — val loss: 0.9189 — val acc: 81.87%\n","Epoch 57/100 — train loss: 1.5360 — train acc: 41.99% — val loss: 0.9062 — val acc: 82.69%\n","Epoch 58/100 — train loss: 1.4757 — train acc: 42.03% — val loss: 0.9031 — val acc: 82.53%\n","Epoch 59/100 — train loss: 1.4985 — train acc: 41.74% — val loss: 0.8758 — val acc: 83.91%\n","Epoch 60/100 — train loss: 1.4761 — train acc: 40.70% — val loss: 0.9107 — val acc: 82.28%\n","Epoch 61/100 — train loss: 1.4959 — train acc: 42.67% — val loss: 0.8840 — val acc: 83.58%\n","Epoch 62/100 — train loss: 1.4940 — train acc: 44.55% — val loss: 0.8755 — val acc: 83.65%\n","Epoch 63/100 — train loss: 1.5050 — train acc: 42.46% — val loss: 0.8800 — val acc: 84.11%\n","Epoch 64/100 — train loss: 1.4790 — train acc: 43.65% — val loss: 0.8917 — val acc: 83.06%\n","Epoch 65/100 — train loss: 1.4644 — train acc: 43.74% — val loss: 0.8929 — val acc: 83.30%\n","Epoch 66/100 — train loss: 1.4425 — train acc: 42.14% — val loss: 0.8778 — val acc: 83.65%\n","Epoch 67/100 — train loss: 1.4307 — train acc: 41.67% — val loss: 0.8688 — val acc: 84.37%\n","Epoch 68/100 — train loss: 1.4545 — train acc: 42.67% — val loss: 0.8739 — val acc: 84.45%\n","Epoch 69/100 — train loss: 1.4434 — train acc: 44.42% — val loss: 0.8568 — val acc: 84.62%\n","Epoch 70/100 — train loss: 1.4527 — train acc: 44.53% — val loss: 0.8551 — val acc: 84.53%\n","Epoch 71/100 — train loss: 1.4530 — train acc: 41.28% — val loss: 0.8571 — val acc: 84.70%\n","Epoch 72/100 — train loss: 1.4063 — train acc: 41.59% — val loss: 0.8545 — val acc: 84.79%\n","Epoch 73/100 — train loss: 1.4543 — train acc: 40.51% — val loss: 0.8527 — val acc: 85.35%\n","Epoch 74/100 — train loss: 1.4335 — train acc: 42.59% — val loss: 0.8424 — val acc: 85.33%\n","Epoch 75/100 — train loss: 1.4097 — train acc: 43.73% — val loss: 0.8454 — val acc: 85.24%\n","Epoch 76/100 — train loss: 1.4251 — train acc: 45.26% — val loss: 0.8439 — val acc: 85.41%\n","Epoch 77/100 — train loss: 1.4506 — train acc: 44.20% — val loss: 0.8417 — val acc: 85.23%\n","Epoch 78/100 — train loss: 1.4439 — train acc: 44.07% — val loss: 0.8486 — val acc: 85.34%\n","Epoch 79/100 — train loss: 1.4266 — train acc: 43.77% — val loss: 0.8403 — val acc: 85.62%\n","Epoch 80/100 — train loss: 1.4257 — train acc: 44.14% — val loss: 0.8300 — val acc: 85.78%\n","Epoch 81/100 — train loss: 1.4179 — train acc: 45.19% — val loss: 0.8300 — val acc: 86.01%\n","Epoch 82/100 — train loss: 1.4287 — train acc: 44.72% — val loss: 0.8323 — val acc: 86.11%\n","Epoch 83/100 — train loss: 1.4483 — train acc: 45.65% — val loss: 0.8352 — val acc: 85.86%\n","Epoch 84/100 — train loss: 1.4413 — train acc: 42.51% — val loss: 0.8226 — val acc: 86.44%\n","Epoch 85/100 — train loss: 1.3969 — train acc: 44.85% — val loss: 0.8194 — val acc: 86.48%\n","Epoch 86/100 — train loss: 1.4028 — train acc: 45.10% — val loss: 0.8242 — val acc: 86.07%\n","Epoch 87/100 — train loss: 1.3995 — train acc: 44.63% — val loss: 0.8227 — val acc: 86.25%\n","Epoch 88/100 — train loss: 1.4158 — train acc: 44.68% — val loss: 0.8245 — val acc: 86.27%\n","Epoch 89/100 — train loss: 1.3956 — train acc: 44.82% — val loss: 0.8229 — val acc: 86.32%\n","Epoch 90/100 — train loss: 1.4016 — train acc: 45.10% — val loss: 0.8202 — val acc: 86.30%\n","Epoch 91/100 — train loss: 1.3934 — train acc: 45.59% — val loss: 0.8162 — val acc: 86.52%\n","Epoch 92/100 — train loss: 1.3981 — train acc: 44.99% — val loss: 0.8171 — val acc: 86.48%\n","Epoch 93/100 — train loss: 1.4400 — train acc: 46.02% — val loss: 0.8205 — val acc: 86.34%\n","Epoch 94/100 — train loss: 1.4172 — train acc: 45.42% — val loss: 0.8197 — val acc: 86.37%\n","Epoch 95/100 — train loss: 1.4120 — train acc: 44.27% — val loss: 0.8200 — val acc: 86.34%\n","Epoch 96/100 — train loss: 1.4017 — train acc: 45.28% — val loss: 0.8183 — val acc: 86.44%\n","Epoch 97/100 — train loss: 1.4002 — train acc: 43.23% — val loss: 0.8174 — val acc: 86.46%\n","Epoch 98/100 — train loss: 1.4178 — train acc: 44.60% — val loss: 0.8177 — val acc: 86.50%\n","Epoch 99/100 — train loss: 1.4289 — train acc: 45.66% — val loss: 0.8180 — val acc: 86.51%\n","Epoch 100/100 — train loss: 1.3904 — train acc: 45.94% — val loss: 0.8179 — val acc: 86.52%\n","Training finished. Best val acc: 86.52%\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"vit_cifar10_best.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"O_RdUsKG252R","executionInfo":{"status":"error","timestamp":1759474901147,"user_tz":-330,"elapsed":39,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"5722d519-e6e9-4528-dca8-ce8a6ced17ed"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'torch' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1048037507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vit_cifar10_best.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","source":["model.eval()\n","correct, total = 0, 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        preds = outputs.argmax(dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","print(f\"Test accuracy: {100.0 * correct / total:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"dZOKNDI_5opJ","executionInfo":{"status":"error","timestamp":1759508868327,"user_tz":-330,"elapsed":829,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"2ff80aab-e514-4168-ce17-d27d14a4fa4a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3837506045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","source":["import os, torch\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader\n","\n","# --- Locate checkpoint ---\n","ckpt_dir = './vit_cifar10_out'\n","ckpt_path = None\n","if os.path.exists(ckpt_dir):\n","    for f in os.listdir(ckpt_dir):\n","        if f.endswith('.pth') or f.endswith('.pt'):\n","            ckpt_path = os.path.join(ckpt_dir, f)\n","            break\n","print(\"Using checkpoint:\", ckpt_path)\n","\n","if ckpt_path is None:\n","    raise FileNotFoundError(\"No checkpoint found in ./vit_cifar10_out. Train or save first.\")\n","\n","# --- Build model (must match training config) ---\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = VisionTransformer(img_size=32, patch_size=4, num_classes=10,\n","                          embed_dim=256, depth=8, num_heads=8,\n","                          mlp_ratio=4.0, drop_rate=0.1, attn_drop_rate=0.0,\n","                          drop_path_rate=0.1).to(device)\n","\n","ckpt = torch.load(ckpt_path, map_location=device)\n","if isinstance(ckpt, dict) and 'model_state' in ckpt:\n","    state = ckpt['model_state']\n","elif isinstance(ckpt, dict) and any(k in ckpt for k in ['state_dict','model_state_dict']):\n","    for k in ['state_dict','model_state','model_state_dict']:\n","        if k in ckpt: state = ckpt[k]; break\n","else:\n","    state = ckpt\n","try:\n","    model.load_state_dict(state)\n","except RuntimeError:\n","    from collections import OrderedDict\n","    new_state = OrderedDict((k.replace('module.',''), v) for k,v in state.items())\n","    model.load_state_dict(new_state)\n","\n","# --- CIFAR-10 test loader ---\n","normalize = transforms.Normalize(mean=[0.4914,0.4822,0.4465], std=[0.247,0.243,0.261])\n","test_transform = transforms.Compose([transforms.ToTensor(), normalize])\n","test_loader = DataLoader(\n","    datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform),\n","    batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n","\n","# --- Evaluate ---\n","model.eval()\n","correct, total = 0, 0\n","with torch.no_grad():\n","    for imgs, labels in test_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        preds = model(imgs).argmax(dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","acc = 100.0 * correct / total\n","print(f\"✅ Test accuracy: {acc:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"saulZ2IQo3-8","executionInfo":{"status":"error","timestamp":1759521248790,"user_tz":-330,"elapsed":69,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"98d26e75-491f-4e9e-95fb-16a8f7e9b19e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using checkpoint: None\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"No checkpoint found in ./vit_cifar10_out. Train or save first.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1774543174.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No checkpoint found in ./vit_cifar10_out. Train or save first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# --- Build model (must match training config) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: No checkpoint found in ./vit_cifar10_out. Train or save first."]}]},{"cell_type":"code","source":["!ls ./checkpoints\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7m2PXvLsbgY","executionInfo":{"status":"ok","timestamp":1759522181376,"user_tz":-330,"elapsed":94,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"cd6282b3-351d-4984-f13d-53eaa19771d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access './checkpoints': No such file or directory\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pra1YQu9ZoWK","executionInfo":{"status":"ok","timestamp":1759601399736,"user_tz":-330,"elapsed":2606,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"f70f4df6-dcf7-4cb0-d42d-136ac6d8d3ef"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaqC1AnUaE9b","executionInfo":{"status":"ok","timestamp":1759601489810,"user_tz":-330,"elapsed":18,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"aa69c5ea-f4a3-4e1d-ffa5-4161ed2d386c"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["!git init"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbXzMHvAbeTK","executionInfo":{"status":"ok","timestamp":1759601633728,"user_tz":-330,"elapsed":321,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"cf4506fe-5180-4c14-c695-8f8f6365b8a5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n","\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n","\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n","\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit branch -m <name>\u001b[m\n","Initialized empty Git repository in /content/drive/MyDrive/Colab Notebooks/.git/\n"]}]},{"cell_type":"code","source":["!git remote add origin https://github.com/devikapmenon/AIRL-Assignment.git"],"metadata":{"id":"JqMoHNILboWR","executionInfo":{"status":"ok","timestamp":1759601673593,"user_tz":-330,"elapsed":72,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["!git add q1.ipynb q2.ipynb README.md\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MPiE9W9cbumO","executionInfo":{"status":"ok","timestamp":1759601685640,"user_tz":-330,"elapsed":116,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"60543aec-305b-4f37-bc06-109b063996e0"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: pathspec 'README.md' did not match any files\n"]}]},{"cell_type":"code","source":["!git commit -m \"Initial commit — CIFAR-10 ViT submission\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKUFjM-HbzUp","executionInfo":{"status":"ok","timestamp":1759601794722,"user_tz":-330,"elapsed":468,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"f5f3aaab-a9c0-4a33-8f98-409283a391a5"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch master\n","\n","Initial commit\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mUntitled\u001b[m\n","\t\u001b[31mUntitled0.ipynb\u001b[m\n","\t\u001b[31mUntitled1.ipynb\u001b[m\n","\t\u001b[31mmodel_training (1).ipynb\u001b[m\n","\t\u001b[31mmodel_training (2).ipynb\u001b[m\n","\t\u001b[31mmodel_training.ipynb\u001b[m\n","\t\u001b[31mq1.ipynb\u001b[m\n","\t\u001b[31mq2 (1).ipynb\u001b[m\n","\t\u001b[31mq2.ipynb\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"aadikrishna4008@gmail.com\"\n","!git config --global user.name \"devikapmenon\""],"metadata":{"id":"Fi90ur_ob15Z","executionInfo":{"status":"ok","timestamp":1759601789067,"user_tz":-330,"elapsed":328,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"Initial commit — CIFAR-10 ViT submission\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2Qp8ywhcY3L","executionInfo":{"status":"ok","timestamp":1759601911609,"user_tz":-330,"elapsed":484,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"db6118ce-de1c-4ab4-d574-51a99ea0b6cc"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch master\n","\n","Initial commit\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mUntitled\u001b[m\n","\t\u001b[31mUntitled0.ipynb\u001b[m\n","\t\u001b[31mUntitled1.ipynb\u001b[m\n","\t\u001b[31mmodel_training (1).ipynb\u001b[m\n","\t\u001b[31mmodel_training (2).ipynb\u001b[m\n","\t\u001b[31mmodel_training.ipynb\u001b[m\n","\t\u001b[31mq1.ipynb\u001b[m\n","\t\u001b[31mq2 (1).ipynb\u001b[m\n","\t\u001b[31mq2.ipynb\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"]}]},{"cell_type":"code","source":["!git branch -M main\n","!git push -u origin main\n"],"metadata":{"id":"UEdyzHvCcq_Z","executionInfo":{"status":"ok","timestamp":1759601933450,"user_tz":-330,"elapsed":231,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"cb25e258-5f6b-48b3-9312-f6bc54d87e53","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["error: src refspec main does not match any\n","\u001b[31merror: failed to push some refs to 'https://github.com/devikapmenon/AIRL-Assignment.git'\n","\u001b[m"]}]},{"cell_type":"code","source":["!git add q1.ipynb q2.ipynb README.md"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"875b5Txrcf7J","executionInfo":{"status":"ok","timestamp":1759601887263,"user_tz":-330,"elapsed":106,"user":{"displayName":"Devika Padmakumar Menon","userId":"09137666478037477257"}},"outputId":"ee4bf9a2-9b92-46f3-86a9-d8f2e1349920"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: pathspec 'README.md' did not match any files\n"]}]}]}